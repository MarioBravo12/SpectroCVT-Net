{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Load libraries"
      ],
      "metadata": {
        "id": "12Kq1luXELUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Concatenate, Lambda, Dense, Dropout, Activation, Flatten, LSTM, SpatialDropout2D, Conv2D, MaxPooling2D,\n",
        "    AveragePooling2D, GlobalAveragePooling2D, UpSampling2D, BatchNormalization, ReLU, Input\n",
        ")\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras import optimizers, regularizers, backend as K\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, accuracy_score, f1_score, recall_score, precision_score, classification_report,\n",
        "    cohen_kappa_score, hamming_loss, log_loss, zero_one_loss, matthews_corrcoef, roc_curve, auc\n",
        ")\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, label_binarize\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from time import time\n",
        "import datetime\n",
        "from skimage.util.shape import view_as_blocks\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import ntpath\n",
        "import copy\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "d7ubk3qVEs3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kcUbYkAi1kr",
        "outputId": "4672fe43-9e60-46aa-84b9-45be713c6204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sys.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4UgQUmSnwo8",
        "outputId": "0892b3fc-e3ac-4dc1-ebeb-9b62df4b0703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "85R-L_pXHM3C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e3b4316-e55b-4f24-8e6c-1abbdaaf3102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Data"
      ],
      "metadata": {
        "id": "9k6kPNDhFJS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load STFT data for each class\n",
        "AD = np.load(\"/content/drive/MyDrive/BrainLat/EEGIMG/1_AD/stft.npy\")  # Alzheimer's Disease (AD)\n",
        "BV = np.load(\"/content/drive/MyDrive/BrainLat/EEGIMG/2_bvFTD/stft.npy\")  # Behavioral variant Frontotemporal Dementia (bvFTD)\n",
        "HC = np.load(\"/content/drive/MyDrive/BrainLat/EEGIMG/5_HC/stft.npy\")  # Healthy Controls (HC)\n",
        "\n",
        "# Concatenate features from all classes along the first axis\n",
        "Features = np.concatenate((AD, BV, HC), axis=0)\n",
        "\n",
        "# Create labels for each class and concatenate them\n",
        "Labels = np.int32(np.concatenate((\n",
        "    0 * np.ones((AD.shape[0])),  # Labels for AD\n",
        "    1 * np.ones((BV.shape[0])),  # Labels for bvFTD\n",
        "    2 * np.ones((HC.shape[0]))   # Labels for HC\n",
        "), axis=0))\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    Features, Labels, test_size=0.2, stratify=Labels, random_state=42\n",
        ")\n",
        "\n",
        "# Print the shapes of the training and test sets\n",
        "print(\"Training data: \", X_train.shape)\n",
        "print(\"Training labels: \", y_train.shape)\n",
        "print(\"Test data: \", X_test.shape)\n",
        "print(\"Test labels: \", y_test.shape)\n",
        "\n",
        "# Convert labels to categorical format for use with neural networks\n",
        "y_train = to_categorical(y_train, 3)\n",
        "y_test = to_categorical(y_test, 3)"
      ],
      "metadata": {
        "id": "MMXj6YVzkn3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions"
      ],
      "metadata": {
        "id": "kyE04oUjFOIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def squeeze_excitation_layer(input_layer, out_dim, ratio, conv):\n",
        "    # Squeeze: Global Average Pooling on the input layer\n",
        "    squeeze = tf.keras.layers.GlobalAveragePooling2D()(input_layer)\n",
        "\n",
        "    # Excitation: Two dense layers with different activations\n",
        "    excitation = tf.keras.layers.Dense(units=out_dim / ratio, activation='relu')(squeeze)\n",
        "    excitation = tf.keras.layers.Dense(out_dim, activation='sigmoid')(excitation)\n",
        "\n",
        "    # Reshape excitation to match input dimensions\n",
        "    excitation = tf.reshape(excitation, [-1, 1, 1, out_dim])\n",
        "\n",
        "    # Scale: Element-wise multiplication of input and excitation\n",
        "    scale = tf.keras.layers.multiply([input_layer, excitation])\n",
        "\n",
        "    if conv:\n",
        "        # Shortcut path with convolution and batch normalization if conv is True\n",
        "        shortcut = tf.keras.layers.Conv2D(out_dim, kernel_size=1, strides=1,\n",
        "                                          padding='same', kernel_initializer='he_normal')(input_layer)\n",
        "        shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
        "    else:\n",
        "        # Shortcut path is the input layer itself if conv is False\n",
        "        shortcut = input_layer\n",
        "\n",
        "    # Output is the addition of the shortcut and scaled input\n",
        "    out = tf.keras.layers.add([shortcut, scale])\n",
        "\n",
        "    return out\n",
        "\n",
        "def sreLu(input):\n",
        "    # Custom ReLU activation with a specified negative slope and no threshold\n",
        "    return ReLU(negative_slope=0.1, threshold=0)(input)\n",
        "\n",
        "def sConv(input, parameters, size, nstrides):\n",
        "    # Convolutional layer with specified parameters, kernel size, strides, and regularization\n",
        "    return Conv2D(parameters, (size, size), strides=(nstrides, nstrides), padding=\"same\",\n",
        "                  kernel_initializer='glorot_normal',\n",
        "                  kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
        "                  bias_regularizer=tf.keras.regularizers.l2(0.0001))(input)\n",
        "\n",
        "def sBN(input):\n",
        "    # Batch normalization with specified momentum and epsilon\n",
        "    return tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=0.001, center=True,\n",
        "                                              scale=True, trainable=True, fused=None,\n",
        "                                              renorm=False, renorm_clipping=None,\n",
        "                                              renorm_momentum=0.4, adjustment=None)(input)\n",
        "\n",
        "def sGlobal_Avg_Pooling(input):\n",
        "    # Global average pooling layer\n",
        "    return tf.keras.layers.GlobalAveragePooling2D()(input)\n",
        "\n",
        "def sDense(input, n_units, activate_c):\n",
        "    # Dense layer with specified number of units and activation function\n",
        "    return tf.keras.layers.Dense(n_units, activation=activate_c)(input)\n",
        "\n",
        "def smultiply(input_1, input_2):\n",
        "    # Element-wise multiplication of two inputs\n",
        "    return tf.keras.layers.multiply([input_1, input_2])\n",
        "\n",
        "def sadd(input_1, input_2):\n",
        "    # Element-wise addition of two inputs\n",
        "    return tf.keras.layers.add([input_1, input_2])\n",
        "\n",
        "# Initial learning rate for the optimizer\n",
        "initial_learning_rate = 0.001\n",
        "\n",
        "# Learning rate schedule with exponential decay\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
        ")"
      ],
      "metadata": {
        "id": "wa4I_2yVFluG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Block of TF\n"
      ],
      "metadata": {
        "id": "VD13NMM1FwOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Block_1(input, parameter):\n",
        "    # First block with convolution, batch normalization, and custom ReLU activation\n",
        "    output = sConv(input, parameter, 3, 1)  # Convolutional layer\n",
        "    output = sBN(output)  # Batch normalization\n",
        "    output = sreLu(output)  # Custom ReLU activation\n",
        "    return output\n",
        "\n",
        "def SE_Block(input, out_dim, ratio):\n",
        "    # Squeeze-and-Excitation block\n",
        "    output = sGlobal_Avg_Pooling(input)  # Global average pooling\n",
        "    output = sDense(output, out_dim / ratio, 'relu')  # Dense layer with ReLU activation\n",
        "    output = sDense(output, out_dim, 'sigmoid')  # Dense layer with sigmoid activation\n",
        "    return output\n",
        "\n",
        "def Block_3(input, parameter):\n",
        "    # Third block with multiple layers and squeeze-and-excitation\n",
        "    addition = sConv(input, parameter, 1, 2)  # Convolutional layer for shortcut path\n",
        "    addition = sBN(addition)  # Batch normalization for shortcut path\n",
        "\n",
        "    output = sConv(input, parameter, 3, 2)  # Convolutional layer\n",
        "    output = sBN(output)  # Batch normalization\n",
        "    output = sreLu(output)  # Custom ReLU activation\n",
        "\n",
        "    output = sConv(output, parameter, 3, 1)  # Another convolutional layer\n",
        "    output = sBN(output)  # Batch normalization\n",
        "\n",
        "    multiplier = SE_Block(output, parameter, parameter)  # Squeeze-and-Excitation block\n",
        "    output = smultiply(multiplier, output)  # Element-wise multiplication with SE block output\n",
        "    output = sadd(output, addition)  # Element-wise addition with shortcut path\n",
        "\n",
        "    return output\n",
        "\n",
        "def Block_4(input, parameter):\n",
        "    # Fourth block with a nested Block_1 and additional layers\n",
        "    output = Block_1(input, parameter)  # Nested Block_1\n",
        "    output = sConv(input, parameter, 3, 1)  # Convolutional layer\n",
        "    output = sBN(output)  # Batch normalization\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "yL2Kn8dJFrw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameters SpectroCVT-Net\n"
      ],
      "metadata": {
        "id": "4dmWVW-bF22N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIMIZER PARAMETERS\n",
        "LEARNING_RATE_2 = 1e-3  # Learning rate for the optimizer\n",
        "WEIGHT_DECAY_2 = 1e-4  # Weight decay for regularization\n",
        "\n",
        "# IMAGE AND PATCH PARAMETERS\n",
        "IMAGE_SIZE_2 = 13  # The input images will be resized to this size (13x13 pixels)\n",
        "PATCH_SIZE_2 = 11  # Size of the patches to be extracted from the input images (11x11 pixels)\n",
        "NUM_PATCHES_2 = (IMAGE_SIZE_2 // PATCH_SIZE_2) ** 2  # Number of patches per image\n",
        "print(NUM_PATCHES_2)\n",
        "\n",
        "# VISION TRANSFORMER ARCHITECTURE PARAMETERS\n",
        "LAYER_NORM_EPS_2 = 1e-6  # Epsilon value for layer normalization\n",
        "PROJECTION_DIM_2 = 128  # Dimensionality of the projection space\n",
        "NUM_HEADS_2 = 4  # Number of attention heads in the multi-head attention layer\n",
        "NUM_LAYERS_2 = 4  # Number of transformer layers\n",
        "MLP_UNITS_2 = [\n",
        "    PROJECTION_DIM_2 * 2,  # Number of units in the first layer of the MLP\n",
        "    PROJECTION_DIM_2       # Number of units in the second layer of the MLP\n",
        "]"
      ],
      "metadata": {
        "id": "yLgmYditGDLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9402847d-bc75-429e-8cec-4ceabdd6237e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions of Convolutional Vision Transformer"
      ],
      "metadata": {
        "id": "KbWdIqMHGXc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def position_embedding(projected_patches, num_patches=NUM_PATCHES_2, projection_dim=PROJECTION_DIM_2):\n",
        "    # Build the positions: create a range of position indices from 0 to num_patches - 1\n",
        "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
        "\n",
        "    # Encode the positions with an Embedding layer\n",
        "    encoded_positions = layers.Embedding(\n",
        "        input_dim=num_patches, output_dim=projection_dim\n",
        "    )(positions)\n",
        "\n",
        "    # Add encoded positions to the projected patches\n",
        "    return projected_patches + encoded_positions\n",
        "\n",
        "def mlp(x, dropout_rate, hidden_units):\n",
        "    # Iterate over the hidden units and add Dense => Dropout layers\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)  # Dense layer with GELU activation\n",
        "        x = layers.Dropout(dropout_rate)(x)  # Dropout layer\n",
        "    return x\n",
        "\n",
        "def transformer_2(encoded_patches):\n",
        "    # Apply layer normalization\n",
        "    x1 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS_2)(encoded_patches)\n",
        "\n",
        "    # Multi-Head Self Attention layer\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=NUM_HEADS_2, key_dim=PROJECTION_DIM_2, dropout=0.1\n",
        "    )(x1, x1)\n",
        "\n",
        "    # Skip connection\n",
        "    x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "    # Apply layer normalization again\n",
        "    x3 = layers.LayerNormalization(epsilon=LAYER_NORM_EPS_2)(x2)\n",
        "\n",
        "    # Apply MLP layer\n",
        "    x4 = mlp(x3, hidden_units=MLP_UNITS_2, dropout_rate=0.1)\n",
        "\n",
        "    # Second skip connection\n",
        "    encoded_patches = layers.Add()([x4, x2])\n",
        "    return encoded_patches\n",
        "\n",
        "def Transform_sh_2(inputs):\n",
        "    # Apply squeeze and excitation layer\n",
        "    inputs1 = squeeze_excitation_layer(inputs, out_dim=512, ratio=32.0, conv=False)\n",
        "    print(inputs1.shape)\n",
        "\n",
        "    # Project input patches using a Conv2D layer\n",
        "    projected_patches = layers.Conv2D(\n",
        "        filters=PROJECTION_DIM_2,\n",
        "        kernel_size=(PATCH_SIZE_2, PATCH_SIZE_2),\n",
        "        strides=(PATCH_SIZE_2, PATCH_SIZE_2),\n",
        "        padding=\"VALID\",\n",
        "    )(inputs1)\n",
        "\n",
        "    # Get the shape of the projected patches\n",
        "    _, h, w, c = projected_patches.shape\n",
        "\n",
        "    # Reshape the projected patches\n",
        "    projected_patches = layers.Reshape((h * w, c))(projected_patches)  # (B, number_patches, projection_dim)\n",
        "\n",
        "    # Add positional embeddings to the projected patches\n",
        "    encoded_patches = position_embedding(projected_patches)  # (B, number_patches, projection_dim)\n",
        "\n",
        "    # Apply dropout\n",
        "    encoded_patches = layers.Dropout(0.1)(encoded_patches)\n",
        "\n",
        "    # Iterate over the number of layers and stack transformer blocks\n",
        "    for i in range(NUM_LAYERS_2):\n",
        "        # Add a transformer block\n",
        "        encoded_patches = transformer_2(encoded_patches)\n",
        "\n",
        "    return encoded_patches"
      ],
      "metadata": {
        "id": "yt5IcLMLGWLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model\n"
      ],
      "metadata": {
        "id": "0ND62NMiDbgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def new_archv():\n",
        "    # Clear any existing TensorFlow graphs\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # Define the input layer with shape (224, 224, 128)\n",
        "    inputs = tf.keras.Input(shape=(224, 224, 128), name=\"input_1\")\n",
        "\n",
        "    # Apply Block_3 with 64 filters\n",
        "    layers = Block_3(inputs, 64)\n",
        "\n",
        "    # Apply Block_3 for 64, 128, and 256 filters sequentially\n",
        "    for i in [64, 128, 256]:\n",
        "        layers = Block_3(layers, i)\n",
        "\n",
        "    # Apply Block_4 with 512 filters\n",
        "    layers = Block_4(layers, 512)\n",
        "\n",
        "    print(layers.shape)\n",
        "\n",
        "    # Apply the Transform_sh_2 function (Vision Transformer part)\n",
        "    CVT1 = Transform_sh_2(layers)\n",
        "\n",
        "    # Apply layer normalization\n",
        "    representation = tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS_2)(CVT1)\n",
        "\n",
        "    # Apply global average pooling\n",
        "    representation = tf.keras.layers.GlobalAvgPool1D()(representation)\n",
        "\n",
        "    # --------------------------------------------------- End of Transformer 2 --------------------------------------------------- #\n",
        "\n",
        "    # Fully connected (FC) layers\n",
        "    layers = Dense(128, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
        "                   bias_regularizer=tf.keras.regularizers.l2(0.0001))(representation)\n",
        "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
        "    layers = BatchNormalization()(layers)\n",
        "\n",
        "    layers = Dense(64, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
        "                   bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
        "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
        "    layers = BatchNormalization()(layers)\n",
        "\n",
        "    layers = Dense(32, kernel_initializer='glorot_normal', kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
        "                   bias_regularizer=tf.keras.regularizers.l2(0.0001))(layers)\n",
        "    layers = ReLU(negative_slope=0.1, threshold=0)(layers)\n",
        "    layers = BatchNormalization()(layers)\n",
        "\n",
        "    # Output layer with softmax activation for classification into 3 classes\n",
        "    predictions = Dense(3, activation=\"softmax\", name=\"output_1\")(layers)\n",
        "\n",
        "    # Create the model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "    # Compile the model if the compile flag is set\n",
        "    if compile:\n",
        "        model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        print(\"Transformer_create\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "KKUGixz9GiTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile de model\n",
        "modelv = new_archv()"
      ],
      "metadata": {
        "id": "JElbmsEtGwp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and metrics"
      ],
      "metadata": {
        "id": "8eEHfwVZGs3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the base paths for logging and image saving\n",
        "path_log_base = '/content/logs'\n",
        "path_img_base = '/content/images'\n",
        "\n",
        "# Create directories if they do not exist\n",
        "if not os.path.exists(path_log_base):\n",
        "    os.makedirs(path_log_base)\n",
        "if not os.path.exists(path_img_base):\n",
        "    os.makedirs(path_img_base)\n",
        "\n",
        "def train(model, X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size, epochs, model_name=\"\"):\n",
        "    # Record the start time for training duration calculation\n",
        "    start_time = tm.time()\n",
        "\n",
        "    # Define the log directory with a timestamp for TensorBoard logs\n",
        "    log_dir = path_log_base + \"/\" + model_name + \"_\" + str(datetime.datetime.now().isoformat()[:19].replace(\"T\", \"_\").replace(\":\", \"-\"))\n",
        "\n",
        "    # Create a TensorBoard callback\n",
        "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n",
        "\n",
        "    # Define the file path for model checkpoints\n",
        "    filepath = log_dir + \"/saved-model-{epoch:03d}-{val_accuracy:.4f}.hdf5\"\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=True, mode='max')\n",
        "\n",
        "    # Reset model states before training\n",
        "    model.reset_states()\n",
        "\n",
        "    # Evaluate the model on test, train, and validation datasets before training\n",
        "    global lossTEST, accuracyTEST, lossTRAIN, accuracyTRAIN, lossVALID, accuracyVALID\n",
        "    lossTEST, accuracyTEST = model.evaluate(X_test, y_test, verbose=None)\n",
        "    lossTRAIN, accuracyTRAIN = model.evaluate(X_train, y_train, verbose=None)\n",
        "    lossVALID, accuracyVALID = model.evaluate(X_valid, y_valid, verbose=None)\n",
        "\n",
        "    # Global variables for storing model history and details\n",
        "    global history, model_Name, log_Dir\n",
        "    model_Name = model_name\n",
        "    log_Dir = log_dir\n",
        "\n",
        "    print(\"Starting the training...\")\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=epochs,\n",
        "                        callbacks=[tensorboard, checkpoint],\n",
        "                        batch_size=batch_size, validation_data=(X_valid, y_valid), verbose=2)\n",
        "\n",
        "    # Evaluate the model on the test set after training\n",
        "    metrics = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    # Calculate and print the time taken for training\n",
        "    TIME = tm.time() - start_time\n",
        "    print(\"Time \" + model_name + \" = %s [seconds]\" % TIME)\n",
        "\n",
        "    # Print the log directory and call the function to log final results\n",
        "    print(\"\\n\")\n",
        "    print(log_dir)\n",
        "    Final_Results_Test(log_dir)\n",
        "\n",
        "    # Return a dictionary of the model metrics\n",
        "    return {k: v for k, v in zip(model.metrics_names, metrics)}\n",
        "\n",
        "def Final_Results_Test(PATH_trained_models):\n",
        "    # Initialize lists for accuracy and loss\n",
        "    global AccTest, LossTest\n",
        "    AccTest = []\n",
        "    LossTest = []\n",
        "\n",
        "    # Initialize variables for best accuracy and corresponding loss\n",
        "    B_accuracy = 0  # B --> Best\n",
        "    for filename in sorted(os.listdir(PATH_trained_models)):\n",
        "        # Skip 'train' and 'validation' files\n",
        "        if filename != 'train' and filename != 'validation':\n",
        "            print(filename)\n",
        "\n",
        "            # Load the model from file\n",
        "            model = tf.keras.models.load_model(PATH_trained_models + '/' + filename, custom_objects={'Tanh3': Tanh3})\n",
        "\n",
        "            # Evaluate the loaded model on the test set\n",
        "            loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "            print(f'Loss={loss:.4f} and Accuracy={accuracy:.4f}' + '\\n')\n",
        "\n",
        "            # Store the accuracy and loss\n",
        "            BandAccTest = accuracy\n",
        "            BandLossTest = loss\n",
        "            AccTest.append(BandAccTest)\n",
        "            LossTest.append(BandLossTest)\n",
        "\n",
        "            # Update best accuracy and corresponding loss\n",
        "            if accuracy > B_accuracy:\n",
        "                B_accuracy = accuracy\n",
        "                B_loss = loss\n",
        "                B_name = filename\n",
        "\n",
        "    # Print the best model results\n",
        "    print(\"\\n\\nBest\")\n",
        "    print(B_name)\n",
        "    print(f'Loss={B_loss:.4f} and Accuracy={B_accuracy:.4f}' + '\\n')"
      ],
      "metadata": {
        "id": "Eq1EyWM7G8U3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with the specified parameters\n",
        "# Train the model with the specified parameters\n",
        "# Arguments:\n",
        "# - modelv: The model to be trained.\n",
        "# - X_train: Training features.\n",
        "# - y_train: Training labels.\n",
        "# - X_test: Testing features (used for both validation and testing here).\n",
        "# - y_test: Testing labels (used for both validation and testing here).\n",
        "# - batch_size: Number of samples per gradient update.\n",
        "# - epochs: Number of epochs to train the model.\n",
        "# - model_name: Name for the model, used to generate log and checkpoint filenames.\n",
        "train(modelv, X_train, y_train, X_test, y_test, X_test, y_test, batch_size=128, epochs=500, model_name=\"Model_Transformer_EEG\")"
      ],
      "metadata": {
        "id": "wqsdUNsLo2m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the best epoch\n",
        "model2=load_model(\"/content/logs/Model_Transformer_EEG_2024-04-19_06-25-56/saved-model-073-0.8889.hdf5\")"
      ],
      "metadata": {
        "id": "hvzSJXgzrph4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set using the trained model\n",
        "y_pred = model2.predict(X_test)  # Predict class probabilities for the test data\n",
        "\n",
        "# Convert predictions to categorical format\n",
        "# np.argmax extracts the class with the highest probability\n",
        "# to_categorical converts these class indices to one-hot encoded vectors\n",
        "y_pred = to_categorical(np.argmax(y_pred, axis=1), num_classes=3)\n",
        "\n",
        "# Calculate performance metrics\n",
        "# Accuracy: Proportion of correctly classified instances\n",
        "accuracy = Accuracy()(y_test, y_pred).numpy()\n",
        "# Precision: Proportion of true positive instances among the predicted positives\n",
        "precision = Precision()(y_test, y_pred).numpy()\n",
        "# Recall: Proportion of true positive instances among the actual positives\n",
        "recall = Recall()(y_test, y_pred).numpy()\n",
        "# AUC: Area under the ROC curve, representing the model's ability to discriminate between classes\n",
        "auc = AUC()(y_test, y_pred).numpy()\n",
        "\n",
        "# Print out the performance metrics\n",
        "print(\"The model's accuracy is: \", accuracy)\n",
        "print(\"The model's precision metric is: \", precision)\n",
        "print(\"The model's recall metric is: \", recall)\n",
        "print(\"The model's AUC is: \", auc)"
      ],
      "metadata": {
        "id": "65lSjvU58dZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the one-hot encoded true labels to class indices if necessary\n",
        "y_true = np.argmax(y_test, axis=1)  # Extract class indices from one-hot encoded y_test\n",
        "\n",
        "# Convert the one-hot encoded predicted labels to class indices if necessary\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Extract class indices from one-hot encoded y_pred\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "# Confusion matrix summarizes the performance of the classification model\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "id": "-xJnbZ8H8dZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the one-hot encoded true labels to class indices if necessary\n",
        "y_true = np.argmax(y_test, axis=1)  # Extract class indices from one-hot encoded y_test\n",
        "\n",
        "# Transform the predicted class indices to one-hot encoded format\n",
        "n_classes = len(np.unique(y_true))  # Number of unique classes\n",
        "y_probs = label_binarize(y_pred.argmax(axis=1), classes=range(n_classes))  # Convert y_pred to one-hot format\n",
        "\n",
        "# Calculate ROC curve and AUC for each class\n",
        "fpr = dict()  # False Positive Rates\n",
        "tpr = dict()  # True Positive Rates\n",
        "roc_auc = dict()  # AUC values\n",
        "for i in range(n_classes):\n",
        "    # Compute ROC curve and AUC for class i\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true == i, y_probs[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Calculate micro-average ROC curve and AUC\n",
        "# Flatten the true labels and predicted probabilities\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(label_binarize(y_true, classes=range(n_classes)).ravel(), y_probs.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "# Plot ROC curves\n",
        "plt.figure(figsize=(8, 8))\n",
        "colors = ['red', 'green', 'blue']  # Colors for each class\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve (class {i}, AUC = {roc_auc[i]:.2f})')\n",
        "\n",
        "# Plot micro-average ROC curve\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='black', lw=2, linestyle='--', label=f'Micro-average ROC curve (AUC = {roc_auc[\"micro\"]:.2f})')\n",
        "\n",
        "# Plot diagonal line for random classifier\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve by Class and Micro-Average')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iyokULsG8dZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class labels for the confusion matrix\n",
        "classes = [\"AD\", \"bvFTD\", \"HC\"]\n",
        "\n",
        "# Plot the confusion matrix using seaborn heatmap\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Reds', cbar=False,\n",
        "            xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predictions')  # Label for the x-axis\n",
        "plt.ylabel('True Labels')  # Label for the y-axis\n",
        "plt.title('Confusion Matrix')  # Title of the plot\n",
        "plt.show()  # Display the plot"
      ],
      "metadata": {
        "id": "0GlhSbqW8dZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the confusion matrix to percentages\n",
        "conf_matrix_percent = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "# Set the font scale for better readability\n",
        "sns.set(font_scale=1.2)\n",
        "\n",
        "# Create a figure with specified size\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot the confusion matrix with percentages\n",
        "sns.heatmap(conf_matrix_percent, annot=True, fmt='.1f', cmap='Reds', cbar=True,\n",
        "            xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predictions')  # Label for the x-axis\n",
        "plt.ylabel('True Labels')  # Label for the y-axis\n",
        "plt.title('Confusion Matrix with Percentages')  # Title of the plot\n",
        "plt.show()  # Display the plot"
      ],
      "metadata": {
        "id": "91SmKKKy8dZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicción del modelo sobre el conjunto de test\n",
        "y_pred = model2.predict(X_test)\n",
        "\n",
        "# Convertir las predicciones de categorías one-hot a etiquetas de clase\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Convertir las etiquetas verdaderas de categorías one-hot a etiquetas de clase si es necesario\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Generar el informe de clasificación\n",
        "# `target_names` especifica los nombres de las clases para que aparezcan en el informe\n",
        "report = classification_report(y_true, y_pred_classes, target_names=['AD', 'bvFTD', 'HC'])\n",
        "\n",
        "# Imprimir el informe de clasificación\n",
        "print(report)"
      ],
      "metadata": {
        "id": "2GEUQ9C7_gz2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}